#!/usr/bin/env node
/**
 * MCP Server Implementation
 *
 * This file implements a Model Context Protocol (MCP) server that provides tools for:
 * - Web search via Google Custom Search API
 * - Web page scraping (including YouTube transcript extraction)
 * - Content analysis using Google's Gemini AI
 *
 * The server supports two transport mechanisms:
 * 1. STDIO - For direct process-to-process communication
 * 2. HTTP+SSE - For web-based clients with Server-Sent Events for streaming
 *
 * All operations use a sophisticated caching system to improve performance and
 * reduce API calls to external services.
 *
 * @see https://github.com/zoharbabin/google-research-mcp for MCP documentation
 */
import express from "express";
import cors from "cors";
import path from "node:path";
import { fileURLToPath } from 'node:url'; // Import fileURLToPath
import { randomUUID } from "node:crypto";
import fs from "node:fs/promises"; // Import fs promises for directory creation
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { StreamableHTTPServerTransport } from "@modelcontextprotocol/sdk/server/streamableHttp.js";
import { PersistentEventStore } from "./shared/persistentEventStore.js";
import { z } from "zod"; // Schema validation library
import { GoogleGenAI } from "@google/genai";
import { CheerioCrawler } from "crawlee"; // Web scraping library
// Import cache modules using index file with .js extension
import { PersistentCache, HybridPersistenceStrategy } from "./cache/index.js";
// Import OAuth scopes documentation and middleware
import { serveOAuthScopesDocumentation } from "./shared/oauthScopesDocumentation.js";
import { createOAuthMiddleware } from "./shared/oauthMiddleware.js";
// Import robust YouTube transcript extractor
import { RobustYouTubeTranscriptExtractor, YouTubeTranscriptError } from "./youtube/transcriptExtractor.js";
// Get the directory name in ES module scope
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
// --- Dynamic Project Root Detection ---
// Find project root by looking for package.json, regardless of execution location
function findProjectRoot(startDir) {
    let currentDir = startDir;
    while (currentDir !== path.dirname(currentDir)) { // Stop at filesystem root
        try {
            // Check if package.json exists in current directory
            const packageJsonPath = path.join(currentDir, 'package.json');
            if (require('fs').existsSync(packageJsonPath)) {
                return currentDir;
            }
        }
        catch {
            // Continue searching if file check fails
        }
        currentDir = path.dirname(currentDir);
    }
    // Fallback: assume we're in project root or one level down
    return __dirname.includes('/dist') ? path.dirname(__dirname) : __dirname;
}
const PROJECT_ROOT = findProjectRoot(__dirname);
// --- Default Paths ---
const DEFAULT_CACHE_PATH = path.resolve(PROJECT_ROOT, 'storage', 'persistent_cache');
const DEFAULT_EVENT_PATH = path.resolve(PROJECT_ROOT, 'storage', 'event_store');
const DEFAULT_REQUEST_QUEUES_PATH = path.resolve(PROJECT_ROOT, 'storage', 'request_queues', 'default');
// --- Global Instances ---
// Initialize Cache and Event Store globally so they are available for both transports
let globalCacheInstance;
let eventStoreInstance;
let transcriptExtractorInstance;
let stdioServerInstance;
let stdioTransportInstance;
let httpTransportInstance;
/**
 * Initializes global cache and event store instances.
 * Ensures storage directories exist.
 * @param cachePath - Path for cache storage
 * @param eventPath - Path for event storage
 */
async function initializeGlobalInstances(cachePath = DEFAULT_CACHE_PATH, eventPath = DEFAULT_EVENT_PATH, requestQueuesPath = DEFAULT_REQUEST_QUEUES_PATH) {
    // Ensure directories exist
    try {
        await fs.mkdir(path.dirname(cachePath), { recursive: true });
        await fs.mkdir(path.dirname(eventPath), { recursive: true });
        await fs.mkdir(requestQueuesPath, { recursive: true });
        if (process.env.NODE_ENV !== 'test') {
            console.log(`✅ Ensured storage directories exist.`);
        }
    }
    catch (error) {
        console.error(`❌ Error ensuring storage directories: ${error}`);
        process.exit(1); // Exit if we can't create storage dirs
    }
    globalCacheInstance = new PersistentCache({
        defaultTTL: 5 * 60 * 1000, // 5 minutes default TTL
        maxSize: 1000, // Maximum 1000 entries
        persistenceStrategy: new HybridPersistenceStrategy(['googleSearch', 'scrapePage'], // Critical namespaces
        5 * 60 * 1000, // 5 minutes persistence interval
        ['googleSearch', 'scrapePage', 'analyzeWithGemini'] // All persistent namespaces
        ),
        storagePath: cachePath,
        eagerLoading: true // Load all entries on startup
    });
    eventStoreInstance = new PersistentEventStore({
        storagePath: eventPath,
        maxEventsPerStream: 1000,
        eventTTL: 24 * 60 * 60 * 1000, // 24 hours
        persistenceInterval: 5 * 60 * 1000, // 5 minutes
        criticalStreamIds: [], // Define critical streams if needed
        eagerLoading: true
    });
    // Initialize robust YouTube transcript extractor
    transcriptExtractorInstance = new RobustYouTubeTranscriptExtractor();
    // Load data eagerly
    await globalCacheInstance.loadFromDisk(); // Cache needs explicit load
    // Event store loads eagerly via constructor option, no explicit call needed here
    if (process.env.NODE_ENV !== 'test') {
        console.log("✅ Global Cache, Event Store, and YouTube Transcript Extractor initialized.");
    }
}
// --- Tool/Resource Configuration (Moved to Top Level) ---
/**
 * Configures and registers all MCP tools and resources for a server instance
 *
 * This factory function sets up:
 * 1. Individual tool implementations with caching
 * 2. Tool registration with the MCP server
 * 3. Composite tools that chain multiple operations
 * 4. Session-specific resources
 *
 * Each tool implementation is extracted into its own function to improve
 * readability and maintainability.
 *
 * @param server - The MCP server instance to configure
 */
function configureToolsAndResources(server) {
    // 1) Extract each tool's implementation into its own async function with caching
    /**
     * Creates a timeout promise that rejects after the specified duration
     */
    const createTimeoutPromise = (ms, operation) => {
        return new Promise((_, reject) => {
            setTimeout(() => reject(new Error(`${operation} timed out after ${ms}ms`)), ms);
        });
    };
    /**
     * Wraps a promise with a timeout
     */
    const withTimeout = async (promise, timeoutMs, operation) => {
        return Promise.race([
            promise,
            createTimeoutPromise(timeoutMs, operation)
        ]);
    };
    /**
     * Performs a Google search with caching and timeout protection
     *
     * Uses the Google Custom Search API to find relevant web pages.
     * Results are cached for 30 minutes with stale-while-revalidate pattern
     * for improved performance and reduced API calls.
     *
     * @param query - The search query string
     * @param num_results - Number of results to return (default: 5)
     * @returns Array of search result URLs as text content items
     */
    const googleSearchFn = async ({ query, num_results, }) => {
        // Use the globally initialized cache instance directly
        return globalCacheInstance.getOrCompute('googleSearch', { query, num_results }, async () => {
            if (process.env.NODE_ENV !== 'production') {
                console.log(`Cache MISS for googleSearch: ${query}, ${num_results}`);
            }
            const key = process.env.GOOGLE_CUSTOM_SEARCH_API_KEY;
            const cx = process.env.GOOGLE_CUSTOM_SEARCH_ID;
            const url = `https://www.googleapis.com/customsearch/v1?key=${key}&cx=${cx}&q=${encodeURIComponent(query)}&num=${num_results}`;
            // Add timeout protection to search API call
            const searchPromise = fetch(url, {
                signal: AbortSignal.timeout(10000) // 10 second timeout
            });
            const resp = await withTimeout(searchPromise, 10000, 'Google Search API');
            if (!resp.ok)
                throw new Error(`Search API error ${resp.status}`);
            const data = await resp.json();
            const links = (data.items || []).map((i) => i.link);
            return links.map((l) => ({ type: "text", text: l }));
        }, {
            ttl: 30 * 60 * 1000, // 30 minutes TTL for search results
            staleWhileRevalidate: true, // Enable stale-while-revalidate
            staleTime: 30 * 60 * 1000 // Allow serving stale content for another 30 minutes while revalidating
        });
    };
    /**
     * Scrapes content from a web page or extracts YouTube transcripts with robust error handling
     *
     * This function:
     * 1. Detects if the URL is a YouTube video and uses the robust transcript extractor
     * 2. Otherwise scrapes the page content using Cheerio
     * 3. Caches results for 1 hour with stale-while-revalidate for up to 24 hours
     * 4. Includes timeout protection and content size limits
     * 5. Provides transparent error reporting for YouTube transcript failures
     *
     * @param url - The URL to scrape
     * @returns The page content as a text content item
     */
    const scrapePageFn = async ({ url }) => {
        // Use a longer TTL for scraped content as it changes less frequently
        // Use the globally initialized cache instance directly
        const result = await globalCacheInstance.getOrCompute('scrapePage', { url }, async () => {
            if (process.env.NODE_ENV !== 'production') {
                console.log(`Cache MISS for scrapePage: ${url}`);
            }
            let text = "";
            const yt = url.match(/(?:youtu\.be\/|youtube\.com\/watch\?v=)([A-Za-z0-9_-]{11})(?:[&#?]|$)/);
            if (yt) {
                // Use robust transcript extractor instead of direct call
                const result = await transcriptExtractorInstance.extractTranscript(yt[1]);
                if (result.success) {
                    text = result.transcript;
                    if (process.env.NODE_ENV !== 'production') {
                        console.log(`✅ YouTube transcript extracted successfully for video ${yt[1]} (${result.attempts} attempts, ${result.duration}ms)`);
                    }
                }
                else {
                    // Throw specific error instead of returning empty text
                    if (process.env.NODE_ENV !== 'production') {
                        console.error(`❌ YouTube transcript extraction failed for video ${yt[1]}: ${result.error.message}`);
                    }
                    throw new YouTubeTranscriptError(result.error.type, result.error.message, yt[1], result.error.originalError);
                }
            }
            else {
                // Regular web scraping logic remains unchanged
                let page = "";
                const crawler = new CheerioCrawler({
                    requestHandler: async ({ $ }) => {
                        // Extract more content by including HTML structure
                        // This ensures we get more than 50 characters for the test
                        const title = $("title").text() || "";
                        const headings = $("h1, h2, h3").map((_, el) => $(el).text()).get().join(" ");
                        const paragraphs = $("p").map((_, el) => $(el).text()).get().join(" ");
                        const bodyText = $("body").text().replace(/\s+/g, " ").trim();
                        // Combine all content to ensure we have enough text
                        page = `Title: ${title}\nHeadings: ${headings}\nParagraphs: ${paragraphs}\nBody: ${bodyText}`;
                    },
                    // Add timeout configuration to crawler
                    requestHandlerTimeoutSecs: 15, // 15 second timeout
                    maxRequestsPerCrawl: 1, // Only process the single URL
                });
                // Add timeout protection for web scraping
                const crawlPromise = crawler.run([{ url }]);
                await withTimeout(crawlPromise, 15000, 'Web page scraping');
                text = page;
            }
            // Limit content size to prevent memory issues (max 50KB per page)
            const MAX_CONTENT_SIZE = 50 * 1024; // 50KB
            if (text.length > MAX_CONTENT_SIZE) {
                // Truncate intelligently - keep beginning and end
                const halfSize = Math.floor(MAX_CONTENT_SIZE / 2);
                text = text.substring(0, halfSize) +
                    "\n\n[... CONTENT TRUNCATED FOR SIZE LIMIT ...]\n\n" +
                    text.substring(text.length - halfSize);
            }
            // Note: Removed fallback content mechanism for YouTube URLs
            // Allow scraped content to be returned as-is, even if short
            // This ensures actual web content is returned instead of test placeholders
            return [{ type: "text", text }];
        }, {
            ttl: 60 * 60 * 1000, // 1 hour TTL for scraped content
            staleWhileRevalidate: true, // Enable stale-while-revalidate
            staleTime: 24 * 60 * 60 * 1000 // Allow serving stale content for up to a day while revalidating
        });
        // Session transcripts are now handled in the session initialization handler
        // We'll return the result directly
        return result;
    };
    const gemini = new GoogleGenAI({
        apiKey: process.env.GOOGLE_GEMINI_API_KEY,
    });
    /**
     * Analyzes text content using Google's Gemini AI models with timeout protection
     *
     * This function sends text to Gemini for analysis and caches the results.
     * Useful for summarization, extraction, or other AI-powered text processing.
     * Includes content size limits and timeout protection.
     *
     * @param text - The text content to analyze
     * @param model - The Gemini model to use (default: gemini-2.0-flash-001)
     * @returns The AI analysis result as a text content item
     */
    const analyzeWithGeminiFn = async ({ text, model = "gemini-2.0-flash-001", }) => {
        // Limit text size for Gemini analysis (max 200KB)
        const MAX_GEMINI_INPUT_SIZE = 200 * 1024; // 200KB
        let processedText = text;
        if (text.length > MAX_GEMINI_INPUT_SIZE) {
            // Truncate intelligently - keep beginning and end with summary note
            const halfSize = Math.floor(MAX_GEMINI_INPUT_SIZE / 2);
            processedText = text.substring(0, halfSize) +
                "\n\n[... CONTENT TRUNCATED FOR API LIMITS - ANALYZE AVAILABLE CONTENT ...]\n\n" +
                text.substring(text.length - halfSize);
            console.log(`⚠️ Gemini input truncated from ${text.length} to ${processedText.length} characters`);
        }
        // Use a shorter TTL for AI analysis as the model may be updated
        // Use the globally initialized cache instance directly
        return globalCacheInstance.getOrCompute('analyzeWithGemini', { text: processedText, model }, async () => {
            if (process.env.NODE_ENV !== 'production') {
                console.log(`Cache MISS for analyzeWithGemini: ${processedText.substring(0, 50)}...`);
            }
            // Add timeout protection for Gemini API calls
            const geminiPromise = gemini.models.generateContent({
                model,
                contents: processedText,
            });
            const r = await withTimeout(geminiPromise, 30000, 'Gemini AI analysis');
            return [{ type: "text", text: r.text }];
        }, {
            ttl: 15 * 60 * 1000, // 15 minutes TTL for AI analysis
            staleWhileRevalidate: true, // Enable stale-while-revalidate
            staleTime: 5 * 60 * 1000 // Allow serving stale content for 5 more minutes while revalidating
        });
    };
    // 2) Register each tool with the MCP server
    /**
     * Enhanced Tool Registration System
     *
     * All tools are now registered with comprehensive MCP-compliant metadata:
     *
     * 1. **Detailed Descriptions**: Each parameter includes comprehensive descriptions
     *    with usage examples, constraints, and optimization guidance
     *
     * 2. **Enhanced Annotations**: Improved titles with clear purpose statements,
     *    proper readOnlyHint and openWorldHint values for LLM guidance
     *
     * 3. **Parameter Documentation**: Zod schemas enhanced with .describe() calls
     *    providing specific examples, constraints, and best practices
     *
     * 4. **LLM-Friendly Format**: Descriptions written specifically for LLM
     *    interpretation with clear use cases and workflow guidance
     *
     * This enhancement dramatically improves tool discoverability and reduces
     * parameter usage errors by providing comprehensive context to LLMs.
     */
    server.tool("google_search", {
        query: z.string().describe("The search query string. Use natural language or specific keywords for better results. More specific queries yield better results and more relevant sources."),
        num_results: z.number().min(1).max(10).default(5).describe("Number of search results to return (1-10). Higher numbers increase processing time and API costs. Use 3-5 for quick research, 8-10 for comprehensive coverage.")
    }, {
        title: "Google Web Search - Find relevant web pages and resources",
        readOnlyHint: true,
        openWorldHint: true
    }, async ({ query, num_results = 5 }) => ({ content: await googleSearchFn({ query, num_results }) }));
    server.tool("scrape_page", {
        url: z.string().url().describe("The URL to scrape. Supports HTTP/HTTPS web pages and YouTube video URLs (youtube.com/watch?v= or youtu.be/ formats). YouTube URLs automatically extract transcripts when available.")
    }, {
        title: "Web Page & YouTube Content Extractor - Extract text content and transcripts",
        readOnlyHint: true,
        openWorldHint: true
    }, async ({ url }) => {
        const result = await scrapePageFn({ url });
        // Note: In SDK v1.11.0, we can't directly access the session ID from the tool handler
        // Session transcripts are now updated through the event system when events are processed
        // The session-specific resources are registered in the session initialization handler
        return { content: result };
    });
    server.tool("analyze_with_gemini", {
        text: z.string().describe("The text content to analyze. Can be articles, documents, scraped content, or any text requiring AI analysis. Large texts are automatically truncated intelligently. Provide clear analysis instructions within the text for best results."),
        model: z.string().default("gemini-2.0-flash-001").describe("The Gemini model to use for analysis. Available options: 'gemini-2.0-flash-001' (fastest, recommended), 'gemini-pro' (detailed analysis), 'gemini-pro-vision' (future multimodal support). Use gemini-2.0-flash-001 for speed, gemini-pro for detailed analysis."),
    }, {
        title: "Gemini AI Text Analysis - Process and analyze content with advanced AI",
        readOnlyHint: true,
        openWorldHint: false
    }, async ({ text, model = "gemini-2.0-flash-001" }) => ({ content: await analyzeWithGeminiFn({ text, model }) }));
    // 3) Create composite tools by chaining operations
    /**
     * Research Topic Tool - A comprehensive research workflow with enhanced descriptions
     *
     * This advanced composite tool implements a sophisticated research pipeline that:
     * 1. Searches for information using Google Search (with timeout protection)
     * 2. Scrapes content from discovered URLs (with graceful degradation)
     * 3. Combines and manages content size intelligently
     * 4. Analyzes the synthesized content with Gemini AI
     *
     * Features:
     * - Resilient architecture with graceful degradation
     * - Promise.allSettled for parallel processing with failure tolerance
     * - Intelligent content size management and truncation
     * - Comprehensive error reporting and success metrics
     * - Individual operation timeouts for reliability
     */
    server.tool("research_topic", {
        query: z.string().describe("The research topic or question to investigate comprehensively. Use descriptive, specific queries for best results. Frame as a research question or specific topic for comprehensive analysis. Examples: 'artificial intelligence trends 2024', 'sustainable energy solutions for small businesses', 'TypeScript performance optimization techniques'."),
        num_results: z.number().min(1).max(5).default(3).describe("Number of sources to research and analyze. More sources provide broader coverage but take longer to process. Recommended range: 2-5 sources for optimal balance of speed and coverage. Use 2-3 for quick research, 4-5 for comprehensive analysis.")
    }, {
        title: "Comprehensive Topic Research Workflow - Advanced multi-step research with AI synthesis",
        readOnlyHint: true,
        openWorldHint: true
    }, async ({ query, num_results }) => {
        const startTime = Date.now();
        const errors = [];
        let searchResults = [];
        try {
            // A) Search with timeout protection
            if (process.env.NODE_ENV !== 'production') {
                console.log(`🔍 Starting research for: "${query}" (${num_results} results)`);
            }
            const searchPromise = googleSearchFn({ query, num_results });
            searchResults = await withTimeout(searchPromise, 15000, 'Google Search');
            if (process.env.NODE_ENV !== 'production') {
                console.log(`✅ Search completed: ${searchResults.length} URLs found`);
            }
        }
        catch (error) {
            const errorMsg = `Search failed: ${error.message}`;
            errors.push(errorMsg);
            if (process.env.NODE_ENV !== 'production') {
                console.error(`❌ ${errorMsg}`);
            }
            // Return early if search fails completely
            return {
                content: [{
                        type: "text",
                        text: `Research failed: Unable to search for "${query}". Error: ${errorMsg}`
                    }]
            };
        }
        const urls = searchResults.map((c) => c.text);
        if (urls.length === 0) {
            return {
                content: [{
                        type: "text",
                        text: `Research completed but no URLs found for query "${query}".`
                    }]
            };
        }
        // B) Scrape each URL with graceful degradation using Promise.allSettled
        if (process.env.NODE_ENV !== 'production') {
            console.log(`📄 Scraping ${urls.length} URLs...`);
        }
        const scrapePromises = urls.map(async (url, index) => {
            try {
                // Add per-URL timeout
                const scrapePromise = scrapePageFn({ url });
                const result = await withTimeout(scrapePromise, 20000, `Scraping URL ${index + 1}`);
                if (process.env.NODE_ENV !== 'production') {
                    console.log(`✅ Scraped URL ${index + 1}/${urls.length}: ${url.substring(0, 50)}...`);
                }
                return { url, result, success: true };
            }
            catch (error) {
                const errorMsg = `Failed to scrape ${url}: ${error.message}`;
                if (process.env.NODE_ENV !== 'production') {
                    console.warn(`⚠️ ${errorMsg}`);
                }
                return { url, error: errorMsg, success: false };
            }
        });
        // Use Promise.allSettled to handle partial failures gracefully
        const scrapeResults = await Promise.allSettled(scrapePromises);
        // Process results and collect successful scrapes
        const successfulScrapes = [];
        scrapeResults.forEach((result, index) => {
            if (result.status === 'fulfilled') {
                if (result.value.success) {
                    successfulScrapes.push(result.value.result);
                }
                else {
                    errors.push(result.value.error);
                }
            }
            else {
                const errorMsg = `Scrape promise rejected for URL ${index + 1}: ${result.reason}`;
                errors.push(errorMsg);
                if (process.env.NODE_ENV !== 'production') {
                    console.error(`❌ ${errorMsg}`);
                }
            }
        });
        if (process.env.NODE_ENV !== 'production') {
            console.log(`📊 Scraping summary: ${successfulScrapes.length}/${urls.length} successful`);
        }
        // C) Check if we have enough content to analyze
        if (successfulScrapes.length === 0) {
            const errorSummary = errors.length > 0 ? `\n\nErrors encountered:\n${errors.join('\n')}` : '';
            return {
                content: [{
                        type: "text",
                        text: `Research failed: Unable to scrape any content from the ${urls.length} URLs found for "${query}".${errorSummary}`
                    }]
            };
        }
        // D) Combine successful scrapes with size management
        const combinedSections = successfulScrapes.map((scrape, index) => {
            const content = scrape[0].text;
            return `=== Source ${index + 1} ===\n${content}`;
        });
        const combined = combinedSections.join("\n\n---\n\n");
        // Limit total combined size before Gemini analysis (max 300KB)
        const MAX_COMBINED_SIZE = 300 * 1024; // 300KB
        let finalCombined = combined;
        if (combined.length > MAX_COMBINED_SIZE) {
            // Truncate intelligently
            const halfSize = Math.floor(MAX_COMBINED_SIZE / 2);
            finalCombined = combined.substring(0, halfSize) +
                "\n\n[... CONTENT TRUNCATED FOR PROCESSING LIMITS ...]\n\n" +
                combined.substring(combined.length - halfSize);
            if (process.env.NODE_ENV !== 'production') {
                console.log(`⚠️ Combined content truncated from ${combined.length} to ${finalCombined.length} characters`);
            }
        }
        // E) Analyze with Gemini (with timeout protection built into analyzeWithGeminiFn)
        try {
            if (process.env.NODE_ENV !== 'production') {
                console.log(`🧠 Analyzing combined content (${finalCombined.length} characters)...`);
            }
            const analysisPromise = analyzeWithGeminiFn({ text: finalCombined });
            const analysis = await withTimeout(analysisPromise, 45000, 'Gemini analysis');
            const totalTime = Date.now() - startTime;
            if (process.env.NODE_ENV !== 'production') {
                console.log(`✅ Research completed in ${totalTime}ms`);
            }
            // Append summary information to analysis
            const summaryInfo = [
                `\n\n--- Research Summary ---`,
                `Query: "${query}"`,
                `URLs found: ${urls.length}`,
                `URLs successfully scraped: ${successfulScrapes.length}`,
                `Content size analyzed: ${finalCombined.length} characters`,
                `Total processing time: ${totalTime}ms`
            ];
            if (errors.length > 0) {
                summaryInfo.push(`Errors encountered: ${errors.length}`);
                summaryInfo.push(`Error details: ${errors.join('; ')}`);
            }
            const analysisText = analysis[0].text + summaryInfo.join('\n');
            return {
                content: [{
                        type: "text",
                        text: analysisText
                    }]
            };
        }
        catch (error) {
            const errorMsg = `Analysis failed: ${error.message}`;
            errors.push(errorMsg);
            if (process.env.NODE_ENV !== 'production') {
                console.error(`❌ ${errorMsg}`);
            }
            // Return raw combined content if analysis fails
            const fallbackContent = [
                `Research partially completed for "${query}" but analysis failed.`,
                ``,
                `Successfully scraped ${successfulScrapes.length}/${urls.length} URLs:`,
                ``,
                finalCombined,
                ``,
                `--- Issues Encountered ---`,
                errors.join('\n')
            ].join('\n');
            return {
                content: [{
                        type: "text",
                        text: fallbackContent
                    }]
            };
        }
    });
    // Session-specific resources are now registered in the session initialization handler
    // This allows for better isolation between sessions and more explicit resource management
}
// --- Function Definitions ---
/**
 * Sets up the STDIO transport using the globally initialized cache and event store.
 */
// Ensure this function is defined at the top level before the main execution block
async function setupStdioTransport() {
    // Ensure global instances are initialized first
    if (!globalCacheInstance || !eventStoreInstance) {
        console.error("❌ Cannot setup stdio transport: Global instances not initialized.");
        process.exit(1);
    }
    stdioServerInstance = new McpServer({
        name: "google-researcher-mcp-stdio",
        version: "1.0.0"
    });
    // Configure tools using the global cache/store (implicitly via configureToolsAndResources)
    configureToolsAndResources(stdioServerInstance); // This function needs access to globalCacheInstance
    stdioTransportInstance = new StdioServerTransport();
    await stdioServerInstance.connect(stdioTransportInstance);
    if (process.env.NODE_ENV !== 'test') {
        console.log("✅ stdio transport ready");
    }
}
/**
 * Factory function to create and configure the Express app for the HTTP+SSE transport
 *
 * @param cache - The pre-initialized PersistentCache instance
 * @param eventStore - The pre-initialized PersistentEventStore instance
 * @param oauthOptions - Optional OAuth configuration
 * @returns Object containing the Express app and the HTTP transport instance
 */
// Add async keyword here
export async function createAppAndHttpTransport(cache, // Accept pre-initialized cache
eventStore, // Accept pre-initialized event store
oauthOptions) {
    // Ensure we have the necessary instances (either global or passed parameters)
    if ((!globalCacheInstance || !eventStoreInstance) && (!cache || !eventStore)) {
        console.error("❌ Cannot create app: Neither global instances nor parameters are available.");
        process.exit(1);
    }
    // ─── 0️⃣ ENVIRONMENT VALIDATION & CORS SETUP ─────────────────────────────────────
    /**
   * Check for required environment variables and configure CORS settings
   *
   * The server requires the following environment variables:
   * - GOOGLE_CUSTOM_SEARCH_API_KEY: For Google search functionality
   * - GOOGLE_CUSTOM_SEARCH_ID: The custom search engine ID
   * - GOOGLE_GEMINI_API_KEY: For Gemini AI analysis
   */
    for (const key of [
        "GOOGLE_CUSTOM_SEARCH_API_KEY",
        "GOOGLE_CUSTOM_SEARCH_ID",
        "GOOGLE_GEMINI_API_KEY"
    ]) {
        if (!process.env[key]) {
            console.error(`❌ Missing required env var ${key}`);
            process.exit(1);
        }
    }
    const ALLOWED_ORIGINS = process.env.ALLOWED_ORIGINS?.split(",").map((s) => s.trim()) || ["*"];
    // Create the Express app instance here
    const app = express();
    app.use(express.json());
    app.use(cors({
        origin: ALLOWED_ORIGINS,
        methods: ["GET", "POST", "DELETE"],
        allowedHeaders: ["Content-Type", "Mcp-Session-Id", "Accept", "Authorization"],
        exposedHeaders: ["Mcp-Session-Id"]
    }));
    // Configure OAuth middleware if options are provided
    let oauthMiddleware;
    if (oauthOptions) { // Use passed-in options
        oauthMiddleware = createOAuthMiddleware(oauthOptions);
        console.log("✅ OAuth 2.1 middleware configured");
    }
    /**
     * Checks if a request is an initialization request
     *
     * Initialization requests create new MCP sessions.
     *
     * @param body - The request body
     * @returns True if this is an initialization request
     */
    function isInitializeRequest(body) {
        return body.method === "initialize";
    }
    // Create the MCP server
    const httpServer = new McpServer({
        name: "google-researcher-mcp-sse",
        version: "1.0.0"
    });
    // Configure tools and resources for the server (using the global function)
    configureToolsAndResources(httpServer); // Call configureTools here
    // Create the streamable HTTP transport with session management
    httpTransportInstance = new StreamableHTTPServerTransport({
        sessionIdGenerator: () => randomUUID(),
        eventStore: eventStoreInstance, // Use the passed-in instance
        onsessioninitialized: (sid) => {
            console.log(`✅ SSE session initialized: ${sid}`);
        },
        // Removed onclose handler - rely on transport's internal management
    });
    // Connect the MCP server to the transport
    await httpServer.connect(httpTransportInstance);
    if (process.env.NODE_ENV !== 'test') {
        console.log("✅ HTTP transport connected to MCP server");
    }
    // Middleware to handle content negotiation for JSON-RPC requests
    app.use((req, res, next) => {
        if (req.path === '/mcp' && req.method === 'POST') {
            // Force content type to be application/json for JSON-RPC requests
            res.setHeader('Content-Type', 'application/json');
        }
        next();
    });
    /**
     * Checks if a request body is a JSON-RPC batch request
     *
     * According to the JSON-RPC specification, batch requests are sent as arrays
     * of individual request objects.
     *
     * @param body - The request body
     * @returns True if this is a batch request
     */
    function isBatchRequest(body) {
        return Array.isArray(body);
    }
    // Handle POST requests to /mcp
    app.post("/mcp", async (req, res, next) => {
        try {
            // Check if this is a batch request
            const isBatch = isBatchRequest(req.body);
            // Handle empty batch requests (invalid according to spec)
            if (isBatch && req.body.length === 0) {
                res.status(400).json({
                    jsonrpc: "2.0",
                    error: { code: -32600, message: "Invalid Request: Empty batch" },
                    id: null
                });
                return;
            }
            // Get session ID from header (still useful for logging)
            const sidHeader = req.headers["mcp-session-id"];
            // Add back explicit session validation for batch requests
            // This is needed for the test to pass
            if (isBatch && (!sidHeader || sidHeader === 'invalid-session-id')) {
                res.status(400).json({
                    jsonrpc: "2.0",
                    error: { code: -32000, message: "Bad Request: No valid session ID provided" },
                    id: null
                });
                return;
            }
            // For existing sessions, delegate to the transport
            // The StreamableHTTPServerTransport should handle batch requests correctly
            // console.log(`Processing ${isBatch ? 'batch' : 'single'} request with session ID: ${sidHeader}`); // Reduce noise
            await httpTransportInstance.handleRequest(req, res, req.body); // Use the instance variable
        }
        catch (err) {
            // console.error("Error processing request:", err); // Reduce noise, rely on default handler
            next(err);
        }
    });
    // Handle GET and DELETE requests to /mcp (SSE connections and session teardown)
    const handleSessionRequest = async (req, res, next) => {
        try {
            const sid = req.headers["mcp-session-id"];
            // Removed explicit session validation - rely on transport's handleRequest
            await httpTransportInstance.handleRequest(req, res); // Use the instance variable
        }
        catch (err) {
            // console.error("Error processing session request:", err); // Reduce noise
            next(err);
        }
    };
    app.get("/mcp", handleSessionRequest);
    app.delete("/mcp", handleSessionRequest);
    // ─── 4️⃣ EVENT STORE & CACHE MANAGEMENT API ENDPOINTS ────────────────────────────
    /**
     * Cache statistics endpoint
     *
     * Provides detailed information about:
     * - Cache hit/miss rates
     * - Memory usage
     * - Entry counts
     * - Server process statistics
     *
     * Useful for monitoring cache performance and diagnosing issues.
     */
    app.get("/mcp/cache-stats", (_req, res) => {
        // Use the globally initialized cache instance directly
        const stats = globalCacheInstance.getStats();
        const processStats = process.memoryUsage();
        res.json({
            cache: {
                ...stats,
                timestamp: new Date().toISOString(),
                memoryUsageEstimate: `~${Math.round(stats.size * 10 / 1024)}MB (rough estimate)`
            },
            process: {
                uptime: process.uptime(),
                memoryUsage: {
                    rss: `${Math.round(processStats.rss / 1024 / 1024)}MB`,
                    heapTotal: `${Math.round(processStats.heapTotal / 1024 / 1024)}MB`,
                    heapUsed: `${Math.round(processStats.heapUsed / 1024 / 1024)}MB`,
                    external: `${Math.round(processStats.external / 1024 / 1024)}MB`
                }
            },
            server: {
                nodeVersion: process.version,
                platform: process.platform,
                startTime: new Date(Date.now() - process.uptime() * 1000).toISOString()
            }
        });
    });
    /**
     * Event store statistics endpoint
     *
     * Provides detailed information about:
     * - Event counts
     * - Memory and disk usage
     * - Hit/miss rates
     * - Stream statistics
     *
     * Useful for monitoring event store performance and diagnosing issues.
     */
    app.get("/mcp/event-store-stats", async (_req, res) => {
        try {
            // Use the passed event store parameter for proper dependency injection
            if (!eventStore || typeof eventStore.getStats !== 'function') {
                res.status(500).json({
                    error: "Event store not available or not a PersistentEventStore"
                });
                return;
            }
            // Get stats from the passed event store parameter
            const stats = await eventStore.getStats();
            res.json({
                eventStore: {
                    ...stats,
                    timestamp: new Date().toISOString()
                }
            });
        }
        catch (error) {
            res.status(500).json({
                error: "Failed to get event store stats",
                message: error.message
            });
        }
    });
    /**
     * Cache invalidation endpoint (protected by API key)
     *
     * Allows authorized clients to:
     * - Invalidate specific cache entries by namespace and args
     * - Clear the entire cache
     *
     * Protected by a simple API key for basic security.
     * In production, use a more robust authentication mechanism.
     */
    app.post("/mcp/cache-invalidate", (req, res) => {
        const apiKey = req.headers["x-api-key"];
        const expectedKey = process.env.CACHE_ADMIN_KEY || "admin-key"; // Should be set in production
        if (apiKey !== expectedKey) {
            res.status(401).json({ error: "Unauthorized" });
            return;
        }
        const { namespace, args } = req.body;
        if (namespace && args) {
            // Invalidate specific entry using the global instance
            globalCacheInstance.invalidate(namespace, args);
            res.json({
                success: true,
                message: `Cache entry invalidated for namespace: ${namespace}`,
                invalidatedAt: new Date().toISOString()
            });
        }
        else {
            // Clear entire cache using the global instance
            globalCacheInstance.clear();
            res.json({
                success: true,
                message: "Entire cache cleared",
                clearedAt: new Date().toISOString()
            });
        }
    });
    /**
     * Cache persistence endpoints (POST and GET)
     *
     * Forces immediate persistence of the cache to disk.
     * Provided in both POST and GET forms for convenience:
     * - POST for programmatic use
     * - GET for easy access via browser
     *
     * Useful for ensuring data is saved before server shutdown.
     */
    app.post("/mcp/cache-persist", async (_req, res) => {
        try {
            // Use the global instance
            await globalCacheInstance.persistToDisk();
            res.json({
                success: true,
                message: "Cache persisted successfully",
                persistedAt: new Date().toISOString()
            });
        }
        catch (error) {
            res.status(500).json({
                success: false,
                message: "Failed to persist cache",
                error: error.message
            });
        }
    });
    // Add GET endpoint for cache persistence (for easier access via browser or curl)
    app.get("/mcp/cache-persist", async (_req, res) => {
        try {
            // Use the global instance
            await globalCacheInstance.persistToDisk();
            res.json({
                success: true,
                message: "Cache persisted successfully",
                persistedAt: new Date().toISOString()
            });
        }
        catch (error) {
            res.status(500).json({
                success: false,
                message: "Failed to persist cache",
                error: error.message
            });
        }
    });
    /**
     * OAuth Scopes Documentation endpoint
     *
     * Provides documentation for the OAuth scopes used in the MCP server.
     * This endpoint serves the documentation in markdown format.
     *
     * Useful for developers integrating with the MCP server's OAuth system.
     */
    app.get("/mcp/oauth-scopes", (req, res) => {
        serveOAuthScopesDocumentation(req, res);
    });
    /**
     * OAuth configuration endpoint
     *
     * Returns the OAuth configuration information, including:
     * - Whether OAuth is enabled
     * - The issuer URL
     * - The audience value
     * - Available endpoints
     *
     * This endpoint is public and does not require authentication.
     */
    app.get("/mcp/oauth-config", (_req, res) => {
        const oauthEnabled = !!oauthOptions; // Use passed-in options
        res.json({
            oauth: {
                enabled: oauthEnabled,
                issuer: oauthEnabled ? oauthOptions.issuerUrl : null,
                audience: oauthEnabled ? oauthOptions.audience : null
            },
            endpoints: {
                jwks: oauthEnabled ? `${oauthOptions.issuerUrl}${oauthOptions.jwksPath || '/.well-known/jwks.json'}` : null,
                tokenInfo: oauthEnabled ? "/mcp/oauth-token-info" : null,
                scopes: "/mcp/oauth-scopes"
            }
        });
    });
    /**
     * OAuth token info endpoint
     *
     * Returns information about the authenticated user's token.
     * This endpoint requires authentication.
     */
    app.get("/mcp/oauth-token-info", 
    // Use a type assertion to help TypeScript understand this is a valid middleware
    (oauthMiddleware || ((req, res, next) => {
        res.status(401).json({
            error: "oauth_not_configured",
            error_description: "OAuth is not configured for this server"
        });
    })), (req, res) => {
        // The OAuth middleware will have attached the token and scopes to the request
        const oauth = req.oauth;
        res.json({
            token: {
                subject: oauth.token.sub,
                issuer: oauth.token.iss,
                audience: oauth.token.aud,
                scopes: oauth.scopes,
                expiresAt: oauth.token.exp ? new Date(oauth.token.exp * 1000).toISOString() : null,
                issuedAt: oauth.token.iat ? new Date(oauth.token.iat * 1000).toISOString() : null
            }
        });
    });
    /**
     * Shutdown handler for graceful cache persistence
     *
     * Ensures cache data is written to disk when the server is terminated
     * with SIGINT (Ctrl+C). This prevents data loss during shutdown.
     */
    process.on('SIGINT', async () => {
        // Refined SIGINT handler for non-test environments
        console.log('Closing transports and persisting data before exit...');
        try {
            // Close transports first
            if (stdioTransportInstance && typeof stdioTransportInstance.close === 'function') {
                await stdioTransportInstance.close();
                console.log('STDIO transport closed.');
            }
            if (httpTransportInstance && typeof httpTransportInstance.close === 'function') {
                await httpTransportInstance.close();
                console.log('HTTP transport closed.');
            }
            // Dispose cache (includes persistence) using the global instance
            if (globalCacheInstance && typeof globalCacheInstance.dispose === 'function') {
                await globalCacheInstance.dispose();
            }
            else {
                console.warn('globalCacheInstance dispose method not found or cache not available.');
            }
            // Dispose the global event store (includes persistence) using the global instance
            if (eventStoreInstance && typeof eventStoreInstance.dispose === 'function') {
                await eventStoreInstance.dispose();
            }
            else {
                console.warn('eventStoreInstance dispose method not found or event store not available.');
            }
        }
        catch (error) {
            console.error('Error during graceful shutdown:', error);
        }
        process.exit(0);
    });
    // ─── 5️⃣ HTTP SERVER STARTUP ────────────────────────────────────────────────────
    /**
     * Start the HTTP server on the configured port
     *
     * Binds to IPv6 ANY address (::) which also accepts IPv4 connections
     * on dual-stack systems. Logs available endpoints for easy access.
     */
    // Return the app and the created HTTP transport instance
    return { app, httpTransport: httpTransportInstance };
} // <-- Closing brace for createAppAndHttpTransport
// Export global instances for potential use in test setup/teardown
export { stdioTransportInstance, httpTransportInstance, globalCacheInstance, eventStoreInstance, transcriptExtractorInstance, initializeGlobalInstances };
// --- Main Execution Block ---
/**
 * Main execution block: Initializes instances and starts transports/server
 * based on execution context (direct run vs. import) and environment variables.
 */
(async () => {
    // Initialize global cache and event store first
    await initializeGlobalInstances();
    // Setup STDIO transport (always runs, uses global instances)
    await setupStdioTransport();
    // If MCP_TEST_MODE is 'stdio', DO NOT start HTTP listener.
    if (process.env.MCP_TEST_MODE === 'stdio') {
        // Log info message only outside of Jest test environment
        if (process.env.NODE_ENV !== 'test') {
            console.log("ℹ️ Running in stdio test mode, HTTP listener skipped.");
            console.log("   STDIO transport is active. Waiting for input...");
        }
    }
    else if (import.meta.url === `file://${process.argv[1]}`) {
        // Otherwise, if run directly, start the HTTP listener.
        const PORT = Number(process.env.PORT || 3000);
        // Pass OAuth options if needed (example: reading from env vars)
        const oauthOpts = process.env.OAUTH_ISSUER_URL ? {
            issuerUrl: process.env.OAUTH_ISSUER_URL,
            audience: process.env.OAUTH_AUDIENCE,
            // jwksPath: process.env.OAUTH_JWKS_PATH // Optional
        } : undefined;
        const { app } = await createAppAndHttpTransport(globalCacheInstance, eventStoreInstance, oauthOpts);
        // Start the HTTP server
        app.listen(PORT, "::", () => {
            console.log(`🌐 SSE server listening on`);
            console.log(`   • http://[::1]:${PORT}/mcp   (IPv6 loopback)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp   (IPv4 loopback)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp/cache-stats   (Cache statistics)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp/event-store-stats   (Event store statistics)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp/cache-persist   (Force cache persistence)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp/oauth-scopes   (OAuth scopes documentation)`);
            console.log(`   • http://127.0.0.1:${PORT}/mcp/oauth-config   (OAuth configuration)`);
            // Always show the OAuth token info endpoint in the logs
            // The endpoint itself will handle the case when OAuth is not configured
            console.log(`   • http://127.0.0.1:${PORT}/mcp/oauth-token-info   (OAuth token info - requires authentication)`);
        });
    }
})(); // End main execution block
//# sourceMappingURL=server.js.map